{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ozyA8W6kvIop"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(filename=\"10AprInfo.log\", level=logging.INFO, format=\"%(asctime)s %(name)s %(message)s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# answer 1\n",
        "A: employee uses health insurance plan\n",
        "B: employee is smoker\n",
        "\n",
        "We need to find the probability of an employee being a smoker given that he/she uses the health insurance plan, which is P(B|A).\n",
        "Given,\n",
        "- 70% of the employees use the health insurance plan, P(A) = 0.7.\n",
        "- 40% of the employees who use the plan are smokers, P(B|A) = 0.4.\n",
        "\n",
        "Bayes' theorem : **P(B|A) = P(A|B) * P(B) / P(A)**\n",
        "\n",
        "Law of total probability:\n",
        "**P(B) = P(B|A) * P(A) + P(B|A') * P(A')**\n",
        "\n",
        "Hence, the probability that an employee is a smoker given that he/she uses the health insurance plan is 40%."
      ],
      "metadata": {
        "id": "6GXT6Xu1yGlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# answer 2\n",
        "Bernoulli Naive Bayes and Multinomial Naive Bayes are two commonly used variants of the Naive Bayes algorithm, which is a probabilistic machine learning algorithm used for classification tasks.\n",
        "\n",
        "The main difference between the two lies in the type of data they are designed to handle.\n",
        "\n",
        "Bernoulli Naive Bayes is typically used when the input data is binary (i.e., when each feature can take on one of two values, such as 0 or 1). In this case, the algorithm calculates the probability of a particular class given the presence or absence of each feature in the input.\n",
        "\n",
        "On the other hand, Multinomial Naive Bayes is typically used when the input data consists of frequency counts of events (i.e., when each feature represents the count of a particular word or token in a text document, for example). In this case, the algorithm calculates the probability of a particular class given the frequency counts of each feature in the input.\n"
      ],
      "metadata": {
        "id": "TtmwsQakyIIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# answer 3\n",
        "Bernoulli Naive Bayes assumes that all features are binary and takes the presence or absence of a feature as a binary value of 1 or 0, respectively. In the case of missing values, the Bernoulli Naive Bayes algorithm can handle them in different ways, depending on the specific implementation.\n",
        "\n",
        "One common approach is to impute the missing values with either 0 or 1, depending on the data and the problem at hand. This can be done using different imputation techniques, such as mean imputation or median imputation, or by using a separate missing value indicator feature, which takes on the value of 1 when a feature is missing and 0 otherwise.\n",
        "\n",
        "Another approach is to simply ignore the missing values and exclude the corresponding samples from the training or testing dataset. This approach can work well if the missing values are relatively rare and the dataset is large enough that removing a few samples does not significantly affect the performance of the model.\n",
        "\n",
        "In any case, it is important to carefully consider the handling of missing values in Bernoulli Naive Bayes and other machine learning algorithms, as the choice of approach can have a significant impact on the performance and accuracy of the model."
      ],
      "metadata": {
        "id": "7QL2YfqYoZ2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# answer 4\n",
        "Yes, Gaussian Naive Bayes can be used for multi-class classification. In this case, the algorithm assumes that the input data follows a Gaussian (normal) distribution and calculates the probability of each class given the input features. The class with the highest probability is then predicted as the output class.\n",
        "\n",
        "For multi-class classification, the algorithm can be extended using one of two common strategies: one-vs-all (also known as one-vs-rest) or one-vs-one.\n",
        "\n",
        "In the one-vs-all strategy, a separate binary classifier is trained for each class, with the samples of that class labeled as positive and the samples of all other classes labeled as negative. The final prediction is made by selecting the class with the highest probability output from all the classifiers.\n",
        "\n",
        "In the one-vs-one strategy, a separate binary classifier is trained for each pair of classes. The final prediction is made by counting the number of votes for each class and selecting the class with the most votes.\n",
        "\n",
        "Overall, Gaussian Naive Bayes is a popular and effective algorithm for multi-class classification, especially when the input features are continuous and normally distributed. However, it may not perform as well as other algorithms, such as logistic regression or support vector machines, for more complex or high-dimensional datasets."
      ],
      "metadata": {
        "id": "Z_B_I1AiohZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# answer 5"
      ],
      "metadata": {
        "id": "pVomMw6_FsMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('spambase.DOCUMENTATION', 'r') as f:\n",
        "    text = f.read()\n",
        "    print(text[0:4000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UKlP0YJQhw6",
        "outputId": "2a886c17-b35d-4972-e6ed-bcc6ef1eeefd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Title:  SPAM E-mail Database\n",
            "\n",
            "2. Sources:\n",
            "   (a) Creators: Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt\n",
            "        Hewlett-Packard Labs, 1501 Page Mill Rd., Palo Alto, CA 94304\n",
            "   (b) Donor: George Forman (gforman at nospam hpl.hp.com)  650-857-7835\n",
            "   (c) Generated: June-July 1999\n",
            "\n",
            "3. Past Usage:\n",
            "   (a) Hewlett-Packard Internal-only Technical Report. External forthcoming.\n",
            "   (b) Determine whether a given email is spam or not.\n",
            "   (c) ~7% misclassification error.\n",
            "       False positives (marking good mail as spam) are very undesirable.\n",
            "       If we insist on zero false positives in the training/testing set,\n",
            "       20-25% of the spam passed through the filter.\n",
            "\n",
            "4. Relevant Information:\n",
            "        The \"spam\" concept is diverse: advertisements for products/web\n",
            "        sites, make money fast schemes, chain letters, pornography...\n",
            "\tOur collection of spam e-mails came from our postmaster and \n",
            "\tindividuals who had filed spam.  Our collection of non-spam \n",
            "\te-mails came from filed work and personal e-mails, and hence\n",
            "\tthe word 'george' and the area code '650' are indicators of \n",
            "\tnon-spam.  These are useful when constructing a personalized \n",
            "\tspam filter.  One would either have to blind such non-spam \n",
            "\tindicators or get a very wide collection of non-spam to \n",
            "\tgenerate a general purpose spam filter.\n",
            "\n",
            "        For background on spam:\n",
            "        Cranor, Lorrie F., LaMacchia, Brian A.  Spam! \n",
            "        Communications of the ACM, 41(8):74-83, 1998.\n",
            "\n",
            "5. Number of Instances: 4601 (1813 Spam = 39.4%)\n",
            "\n",
            "6. Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
            "\n",
            "7. Attribute Information:\n",
            "The last column of 'spambase.data' denotes whether the e-mail was \n",
            "considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \n",
            "Most of the attributes indicate whether a particular word or\n",
            "character was frequently occuring in the e-mail.  The run-length\n",
            "attributes (55-57) measure the length of sequences of consecutive \n",
            "capital letters.  For the statistical measures of each attribute, \n",
            "see the end of this file.  Here are the definitions of the attributes:\n",
            "\n",
            "48 continuous real [0,100] attributes of type word_freq_WORD \n",
            "= percentage of words in the e-mail that match WORD,\n",
            "i.e. 100 * (number of times the WORD appears in the e-mail) / \n",
            "total number of words in e-mail.  A \"word\" in this case is any \n",
            "string of alphanumeric characters bounded by non-alphanumeric \n",
            "characters or end-of-string.\n",
            "\n",
            "6 continuous real [0,100] attributes of type char_freq_CHAR\n",
            "= percentage of characters in the e-mail that match CHAR,\n",
            "i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
            "\n",
            "1 continuous real [1,...] attribute of type capital_run_length_average\n",
            "= average length of uninterrupted sequences of capital letters\n",
            "\n",
            "1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
            "= length of longest uninterrupted sequence of capital letters\n",
            "\n",
            "1 continuous integer [1,...] attribute of type capital_run_length_total\n",
            "= sum of length of uninterrupted sequences of capital letters\n",
            "= total number of capital letters in the e-mail\n",
            "\n",
            "1 nominal {0,1} class attribute of type spam\n",
            "= denotes whether the e-mail was considered spam (1) or not (0), \n",
            "i.e. unsolicited commercial e-mail.  \n",
            "\n",
            "\n",
            "8. Missing Attribute Values: None\n",
            "\n",
            "9. Class Distribution:\n",
            "\tSpam\t  1813  (39.4%)\n",
            "\tNon-Spam  2788  (60.6%)\n",
            "\n",
            "\n",
            "Attribute Statistics:\n",
            "   Min: Max:   Average:  Std.Dev: Coeff.Var_%: \n",
            "1  0    4.54   0.10455   0.30536  292          \n",
            "2  0    14.28  0.21301   1.2906   606          \n",
            "3  0    5.1    0.28066   0.50414  180          \n",
            "4  0    42.81  0.065425  1.3952   2130         \n",
            "5  0    10     0.31222   0.67251  215          \n",
            "6  0    5.88   0.095901  0.27382  286          \n",
            "7  0    7.27   0.11421   0.39144  343          \n",
            "8  0    11.11  0.10529   0.40107  381          \n",
            "9  0    5.26   0.090067  0.27862  309          \n",
            "10 0    18.18  0.23941   0.64476  269          \n",
            "11 0    2.61   0.059824  0.20154  337          \n",
            "12 0    9.67   0.5417    0.8617   159          \n",
            "13 0    5.55   0.09393   0.30104  320          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# load the dataset\n",
        "data = np.loadtxt('spambase.data', delimiter=',')\n",
        "\n",
        "# split the dataset into features (X) and labels (y)\n",
        "X = data[:, :-1]\n",
        "y = data[:, -1]\n",
        "\n",
        "# create Bernoulli Naive Bayes classifier and evaluate performance using 10-fold cross-validation\n",
        "bnb = BernoulliNB()\n",
        "bnb_y_pred = cross_val_predict(bnb, X, y, cv=10)\n",
        "bnb_accuracy = accuracy_score(y, bnb_y_pred)\n",
        "bnb_precision = precision_score(y, bnb_y_pred)\n",
        "bnb_recall = recall_score(y, bnb_y_pred)\n",
        "bnb_f1_score = f1_score(y, bnb_y_pred)\n",
        "print(\"Bernoulli Naive Bayes performance metrics:\")\n",
        "print(\"Accuracy:\", bnb_accuracy)\n",
        "print(\"Precision:\", bnb_precision)\n",
        "print(\"Recall:\", bnb_recall)\n",
        "print(\"F1 score:\", bnb_f1_score)\n",
        "\n",
        "# create Multinomial Naive Bayes classifier and evaluate performance using 10-fold cross-validation\n",
        "mnb = MultinomialNB()\n",
        "mnb_y_pred = cross_val_predict(mnb, X, y, cv=10)\n",
        "mnb_accuracy = accuracy_score(y, mnb_y_pred)\n",
        "mnb_precision = precision_score(y, mnb_y_pred)\n",
        "mnb_recall = recall_score(y, mnb_y_pred)\n",
        "mnb_f1_score = f1_score(y, mnb_y_pred)\n",
        "print(\"Multinomial Naive Bayes performance metrics:\")\n",
        "print(\"Accuracy:\", mnb_accuracy)\n",
        "print(\"Precision:\", mnb_precision)\n",
        "print(\"Recall:\", mnb_recall)\n",
        "print(\"F1 score:\", mnb_f1_score)\n",
        "\n",
        "# create Gaussian Naive Bayes classifier and evaluate performance using 10-fold cross-validation\n",
        "gnb = GaussianNB()\n",
        "gnb_y_pred = cross_val_predict(gnb, X, y, cv=10)\n",
        "gnb_accuracy = accuracy_score(y, gnb_y_pred)\n",
        "gnb_precision = precision_score(y, gnb_y_pred)\n",
        "gnb_recall = recall_score(y, gnb_y_pred)\n",
        "gnb_f1_score = f1_score(y, gnb_y_pred)\n",
        "print(\"Gaussian Naive Bayes performance metrics:\")\n",
        "print(\"Accuracy:\", gnb_accuracy)\n",
        "print(\"Precision:\", gnb_precision)\n",
        "print(\"Recall:\", gnb_recall)\n",
        "print(\"F1 score:\", gnb_f1_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzwncXlFvCtJ",
        "outputId": "b0e189c8-5041-42f6-a734-024f61dcea19"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bernoulli Naive Bayes performance metrics:\n",
            "Accuracy: 0.8839382742881983\n",
            "Precision: 0.8813357185450209\n",
            "Recall: 0.815223386651958\n",
            "F1 score: 0.8469914040114614\n",
            "Multinomial Naive Bayes performance metrics:\n",
            "Accuracy: 0.786350793305803\n",
            "Precision: 0.7323628219484882\n",
            "Recall: 0.7214561500275786\n",
            "F1 score: 0.7268685746040567\n",
            "Gaussian Naive Bayes performance metrics:\n",
            "Accuracy: 0.8217778743751358\n",
            "Precision: 0.7004440855874041\n",
            "Recall: 0.9569773855488142\n",
            "F1 score: 0.8088578088578089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion:\n",
        "The performance metrics reported above show the performance of three different types of Naive Bayes classifiers (Bernoulli, Multinomial, and Gaussian) on the spambase dataset.\n",
        "\n",
        "- The Bernoulli Naive Bayes classifier achieved the highest accuracy score of 0.8839, which means that it correctly classified 88.39% of the instances in the dataset. It also achieved high precision and F1 scores, which indicates that it had a low false positive rate and balanced precision and recall. However, its recall score of 0.8152 is relatively lower, indicating that it missed some instances that were actually spam.\n",
        "\n",
        "- The Multinomial Naive Bayes classifier achieved an accuracy score of 0.7863, which is lower than the Bernoulli classifier. It also has lower precision, recall, and F1 scores than the Bernoulli classifier. This indicates that the Multinomial classifier is not as effective as the Bernoulli classifier in classifying spam.\n",
        "\n",
        "- The Gaussian Naive Bayes classifier achieved an accuracy score of 0.8218, which is between the Bernoulli and Multinomial classifiers. It has a high recall score of 0.9570, which means it correctly classified almost all spam instances. However, its precision score is lower, indicating that it may have a higher false positive rate than the Bernoulli classifier."
      ],
      "metadata": {
        "id": "KFGStBtR1b9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "The Bernoulli Naive Bayes classifier is the most effective classifier for this particular dataset, achieving the highest accuracy, precision, and F1 scores. The Multinomial and Gaussian Naive Bayes classifiers are not as effective as the Bernoulli classifier in classifying spam. However, the Gaussian Naive Bayes classifier achieved a very high recall score, indicating that it may be useful in identifying most spam instances, although it may generate more false positives than the Bernoulli classifier."
      ],
      "metadata": {
        "id": "bPGnema31hZ9"
      }
    }
  ]
}