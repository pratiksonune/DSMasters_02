{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS-wIKacADk-"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(filename=\"17AprInfo.log\", level=logging.INFO, format=\"%(asctime)s %(name)s %(message)s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# answer 1\n",
        "Gradient Boosting Regression is a popular machine learning algorithm used for supervised regression tasks. It is an ensemble learning method that combines the predictions of several weak learners to create a more accurate and robust model.\n",
        "\n",
        "The basic idea behind gradient boosting is to iteratively train a sequence of simple models, usually decision trees, and add them to the ensemble. Each new model is trained on the residual errors of the previous models, with the goal of reducing the overall error of the ensemble.\n",
        "\n",
        "In each iteration, the algorithm calculates the negative gradient of the loss function with respect to the predicted values and fits a new model to this residual. The predicted values of the new model are then added to the predictions of the previous models, and the process is repeated until the specified number of models is reached, or until the error is minimized to a satisfactory level.\n",
        "\n",
        "The hyperparameters of the algorithm, such as the learning rate, the number of trees, and the maximum depth of the trees, can be tuned to optimize performance. Gradient Boosting Regression has been shown to be effective in a wide range of applications, including finance, marketing, and computer vision."
      ],
      "metadata": {
        "id": "Qthy_c3_AdQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# answer 2\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.y_mean = np.mean(y)\n",
        "        y_pred = np.full_like(y, self.y_mean)\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            residuals = y - y_pred\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.full(X.shape[0], self.y_mean)\n",
        "        for tree in self.trees:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "xjg0LoWJBh7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "# from decision_tree_regressor import DecisionTreeRegressor # Assume we have implemented DecisionTreeRegressor elsewhere\n",
        "\n",
        "# Generate a small regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize a GradientBoostingRegressor model\n",
        "gbm = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# Fit the model to the training data\n",
        "gbm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = gbm.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model using mean squared error and R-squared\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8zXhS1OFdLO",
        "outputId": "b27a55ef-9dbe-4c2b-c7d4-f26ba33e2531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 3069.37\n",
            "R-squared: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# answer 3\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Generate a small regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter space\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Initialize a GradientBoostingRegressor model\n",
        "gbm = GradientBoostingRegressor()\n",
        "\n",
        "# Initialize a GridSearchCV object\n",
        "grid_search = GridSearchCV(gbm, param_grid, cv=3, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the best model using mean squared error and R-squared\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm9GTnXNIQoW",
        "outputId": "b8cc2879-a0f1-448b-f342-fdec156bb727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\n",
            "Mean Squared Error: 3010.96\n",
            "R-squared: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# answer 4\n",
        "In Gradient Boosting, a weak learner is a model that performs only slightly better than random guessing on a given task. In the context of regression problems, a common weak learner used in Gradient Boosting is a decision tree with a small depth.\n",
        "\n",
        "The idea behind Gradient Boosting is to iteratively improve the performance of the model by adding weak learners to the ensemble. In each iteration, the weak learner is trained on the residual errors of the previous ensemble. The residual errors represent the difference between the true values and the predictions made by the previous ensemble. By focusing on the residual errors, the subsequent weak learners are able to improve the performance of the model.\n",
        "\n",
        "Because the weak learners are relatively simple models, they are typically faster to train and require less computational resources than more complex models. However, the strength of the ensemble comes from the combination of many weak learners, each one improving the overall performance slightly. This is why Gradient Boosting is often referred to as a \"slow and steady\" algorithm, since it builds the model iteratively and gradually improves its performance over time."
      ],
      "metadata": {
        "id": "eoiuOT1KBlE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# answer 5\n",
        "The intuition behind Gradient Boosting algorithm can be understood by breaking down the name into its two parts: \"Gradient\" and \"Boosting\".\n",
        "\n",
        "\"Gradient\" refers to the fact that the algorithm uses the gradient of the loss function to determine the direction and magnitude of the updates to the model parameters. In other words, the algorithm tries to minimize the loss function by iteratively adjusting the model parameters in the direction that reduces the loss the most. This is done by computing the gradients of the loss function with respect to the model parameters, and using these gradients to update the model in a way that minimizes the loss.\n",
        "\n",
        "\"Boosting\" refers to the fact that the algorithm builds an ensemble of weak learners, and \"boosts\" the performance of the ensemble by iteratively adding new weak learners that focus on the residual errors of the previous ensemble. In other words, the algorithm builds a series of models, each one improving on the performance of the previous model, until the ensemble achieves the desired level of accuracy."
      ],
      "metadata": {
        "id": "d2Y9eyjCBtqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# answer 6\n",
        "The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding new models to the ensemble, each one focusing on the residual errors of the previous models.\n",
        "\n",
        "- Here are the high-level steps of how Gradient Boosting algorithm builds an ensemble of weak learners:\n",
        "\n",
        "1. Initialize the ensemble with a simple model that predicts the mean value of the target variable.\n",
        "\n",
        "2. Compute the residuals (i.e., the differences between the true target values and the predictions made by the current ensemble) for each training example.\n",
        "\n",
        "3. Train a new weak learner (e.g., decision tree with limited depth) on the residuals from step 2. The weak learner should be trained to predict the residuals.\n",
        "\n",
        "4. Add the new weak learner to the ensemble, and update the predictions of the ensemble by adding the predictions of the new weak learner multiplied by a small learning rate (e.g., 0.1) to the predictions of the previous ensemble.\n",
        "\n",
        "5. Repeat steps 2-4 for a fixed number of iterations (i.e., until a stopping criterion is met) or until the performance of the ensemble stops improving.\n",
        "\n",
        "**The intuition behind this process is that each weak learner is trying to capture the patterns in the residuals that were not captured by the previous models in the ensemble. By doing this, each new model in the ensemble helps to reduce the overall error of the ensemble. The learning rate controls the contribution of each new weak learner to the ensemble. A smaller learning rate gives more weight to the previous models in the ensemble, while a larger learning rate gives more weight to the new weak learner.**"
      ],
      "metadata": {
        "id": "hpoi9013B4BX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# answer 7\n",
        "The mathematical intuition behind Gradient Boosting algorithm can be broken down into the following steps:\n",
        "\n",
        "1. Define a loss function that measures the difference between the true target values and the predicted target values. The loss function typically takes the form of the mean squared error or some other measure of the difference between the true and predicted values.\n",
        "\n",
        "2. Initialize the model with a simple estimator that predicts the mean value of the target variable for all examples in the training set.\n",
        "\n",
        "3. Compute the negative gradient of the loss function with respect to the predicted target values. This gives us a measure of how much we need to adjust the predicted target values to minimize the loss function.\n",
        "\n",
        "4. Train a new estimator (e.g., decision tree with limited depth) to predict the negative gradient computed in step 3. The new estimator is trained on the original input features and the negative gradient as the target variable.\n",
        "\n",
        "5. Add the new estimator to the model by combining its predictions with the predictions of the previous estimators using a small learning rate. The learning rate determines the contribution of the new estimator to the final predictions.\n",
        "\n",
        "6. Repeat steps 3-5 for a fixed number of iterations or until the performance of the model stops improving.\n",
        "\n",
        "**The intuition behind these steps is that each new estimator is trained to predict the errors of the previous estimators, and by combining the predictions of all the estimators, we are able to gradually improve the performance of the model. The negative gradient of the loss function is used to guide the training of the new estimators, ensuring that they focus on the areas where the previous estimators made errors. The learning rate controls the contribution of each new estimator to the final predictions, allowing us to balance the contribution of the new estimators with the contribution of the previous estimators.**"
      ],
      "metadata": {
        "id": "4YxNsjQnCHNl"
      }
    }
  ]
}